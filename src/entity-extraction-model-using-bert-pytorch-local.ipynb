{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#New-class\" data-toc-modified-id=\"New-class-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>New class</a></span><ul class=\"toc-item\"><li><span><a href=\"#config\" data-toc-modified-id=\"config-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>config</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Definitions\" data-toc-modified-id=\"Definitions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Definitions</a></span></li></ul></li><li><span><a href=\"#Old-runs\" data-toc-modified-id=\"Old-runs-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Old runs</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T18:50:34.946209Z",
     "start_time": "2020-09-08T18:50:29.585755Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union, NamedTuple\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask_ml.model_selection import train_test_split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "from copy import deepcopy\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from pathlib import Path\n",
    "from spacy.gold import docs_to_json, biluo_tags_from_offsets, spans_from_biluo_tags, iob_to_biluo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T18:51:56.802793Z",
     "start_time": "2020-09-08T18:51:56.791780Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.tokens.span import Span as SpacySpan\n",
    "def combine_overlapping(segments: List[Tuple[int, int]]) -> Iterable[Tuple[int, int]]:\n",
    "    segments = sorted(set(segments))\n",
    "    combined = None\n",
    "    for segment in segments:\n",
    "        if combined is None:\n",
    "            combined = segment\n",
    "        elif combined[0] <= segment[0] <= combined[1] + 1:\n",
    "            combined = (combined[0], max(segment[1], combined[1]))\n",
    "        else:\n",
    "            yield combined\n",
    "            combined = segment\n",
    "    if combined is not None:\n",
    "        yield combined\n",
    "\n",
    "\n",
    "def maximal_spans(spans: List[SpacySpan]) -> List[SpacySpan]:\n",
    "    if not spans or len(spans) < 1 :\n",
    "        return []\n",
    "    document = spans[0].doc\n",
    "    segments = [(span.start_char, span.end_char) for span in spans]\n",
    "    maximal = [document.char_span(start_char, end_char) for start_char, end_char in combine_overlapping(segments)]\n",
    "    return sorted(maximal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T18:51:57.712537Z",
     "start_time": "2020-09-08T18:51:57.706213Z"
    }
   },
   "outputs": [],
   "source": [
    "def biluo_to_iob(tags):\n",
    "    out = []\n",
    "    for tag in tags:\n",
    "        if tag is None:\n",
    "            out.append(tag)\n",
    "        else:\n",
    "            tag = tag.replace(\"U-\", \"B-\", 1).replace(\"L-\", \"I-\", 1)\n",
    "            out.append(tag)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New class\n",
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T18:51:58.344221Z",
     "start_time": "2020-09-08T18:51:58.339326Z"
    }
   },
   "outputs": [],
   "source": [
    "class AnnotatedText(NamedTuple):\n",
    "    text: str\n",
    "    start: int\n",
    "    end: int\n",
    "class AnnotatedTextLabeled(NamedTuple):\n",
    "    text: str\n",
    "    start: int\n",
    "    end: int\n",
    "    label: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T18:51:58.748698Z",
     "start_time": "2020-09-08T18:51:58.743304Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    max_len = 128,\n",
    "    train_batch_size = 32,\n",
    "    valid_batch_size = 8,\n",
    "    iterations = 3,\n",
    "    model_name = \"bert-base-uncased\",\n",
    "    training_file = \"/src/ner_dataset.csv\",\n",
    "    path_to_model = './model.bin'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T19:34:38.101253Z",
     "start_time": "2020-09-08T19:25:30.456525Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(config.get('training_file'), encoding=\"latin-1\")\n",
    "df.loc[:, \"Sentence #\"]=df[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "df=df.groupby(\"Sentence #\").agg({\"Tag\": list, \"Word\": lambda x: ' '.join(x)}).reset_index()\n",
    "df['Tag']=df['Tag'].apply(iob_to_biluo)#.apply(lambda y: list(map(lambda x: x.upper(), y)))\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "df['Tags']=df[['Word', 'Tag']].apply(lambda x: spans_from_biluo_tags(nlp(x['Word']), x['Tag']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T19:34:40.039655Z",
     "start_time": "2020-09-08T19:34:38.367222Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['AnnotationLabeled']=df['Tags'].apply(lambda y: list(map(lambda x: AnnotatedTextLabeled(x.text,\n",
    "                                                                                 x.start_char,\n",
    "                                                                                 x.end_char,\n",
    "                                                                                 x.label_\n",
    "                                                                                ), y)))\n",
    "df['Annotation']=df['Tags'].apply(lambda y: list(map(lambda x: AnnotatedText(x.text,\n",
    "                                                                                 x.start_char,\n",
    "                                                                                 x.end_char,\n",
    "                                                                                ), y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T19:34:40.382654Z",
     "start_time": "2020-09-08T19:34:40.379095Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47959, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.drop(['Tag', 'Tags'], axis=1, inplace=True, errors='ignore')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T21:40:59.980568Z",
     "start_time": "2020-09-08T21:40:59.827008Z"
    },
    "code_folding": [
     0,
     18,
     59,
     140,
     254,
     269,
     279
    ]
   },
   "outputs": [],
   "source": [
    "def group_sub_entities(entities: List[dict], tokenizer) -> dict:\n",
    "    \"\"\"\n",
    "    Group together the adjacent tokens with the same entity predicted.\n",
    "    Args:\n",
    "        entities (:obj:`dict`): The entities predicted by the pipeline.\n",
    "    \"\"\"\n",
    "    # Get the first entity in the entity group\n",
    "    entity = entities[0][\"entity\"]\n",
    "#     scores = np.mean([entity[\"score\"] for entity in entities])\n",
    "    tokens = [entity[\"word\"] for entity in entities]\n",
    "\n",
    "    entity_group = {\n",
    "        \"entity_group\": entity,\n",
    "#         \"score\": np.mean(scores),\n",
    "        \"word\": tokenizer.convert_tokens_to_string(tokens),\n",
    "    }\n",
    "    return entity_group\n",
    "\n",
    "def group_entities(entities: List[dict], tokenizer) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Find and group together the adjacent tokens with the same entity predicted.\n",
    "    Args:\n",
    "        entities (:obj:`dict`): The entities predicted by the pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    entity_groups = []\n",
    "    entity_group_disagg = []\n",
    "\n",
    "    if entities:\n",
    "        last_idx = entities[-1][\"index\"]\n",
    "\n",
    "    for entity in entities:\n",
    "        is_last_idx = entity[\"index\"] == last_idx\n",
    "        if not entity_group_disagg:\n",
    "            entity_group_disagg += [entity]\n",
    "            if is_last_idx:\n",
    "                entity_groups += [group_sub_entities(entity_group_disagg, tokenizer)]\n",
    "            continue\n",
    "\n",
    "        # If the current entity is similar and adjacent to the previous entity, append it to the disaggregated entity group\n",
    "        # The split is meant to account for the \"B\" and \"I\" suffixes\n",
    "        if (\n",
    "            entity[\"entity\"].split(\"-\")[-1] == entity_group_disagg[-1][\"entity\"].split(\"-\")[-1]\n",
    "            and entity[\"index\"] == entity_group_disagg[-1][\"index\"] + 1\n",
    "        ):\n",
    "            entity_group_disagg += [entity]\n",
    "            # Group the entities at the last entity\n",
    "            if is_last_idx:\n",
    "                entity_groups += [group_sub_entities(entity_group_disagg, tokenizer)]\n",
    "        # If the current entity is different from the previous entity, aggregate the disaggregated entity group\n",
    "        else:\n",
    "            entity_groups += [group_sub_entities(entity_group_disagg, tokenizer)]\n",
    "            entity_group_disagg = [entity]\n",
    "            # If it's the last entity, add it to the entity groups\n",
    "            if is_last_idx:\n",
    "                entity_groups += [group_sub_entities(entity_group_disagg, tokenizer)]\n",
    "\n",
    "    return entity_groups\n",
    "\n",
    "class EntityLabel(NamedTuple):\n",
    "    start: int\n",
    "    end: int\n",
    "    label: Union[str, int]\n",
    "\n",
    "class EntityDataset:\n",
    "    \"\"\"\n",
    "    This class is used with torch.utils.data.DataLoader. This class accepts dask dataframe as input.\n",
    "    pytorch handles when to ask for an item from this class and which item to fetch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ddf: dd,\n",
    "                 text_column: str,\n",
    "                 label_column: str,\n",
    "                 max_len: int,\n",
    "                 tokenizer,\n",
    "                 predict_mode=False\n",
    "                 ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param ddf: Dask dataframe\n",
    "        :param text_column: name of the column that contains the text to classify\n",
    "        :param label_column: name of the column that contains labels\n",
    "                             entries inside label_column need to be of format `EntityLabel`\n",
    "                             if there are more than one type of entities then `label` of\n",
    "                             each tuple should be int but if only single entity then the\n",
    "                             label really doesn't matter\n",
    "        :param max_len: maximum length of a sentence to do padding or truncating\n",
    "        \"\"\"\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.predict_mode = predict_mode\n",
    "        self.texts=ddf[self.text_column].compute().values\n",
    "        self.labels=ddf[self.label_column].compute().values  \n",
    "        self.size = len(self.texts)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # get row number `item` from dask dataframe\n",
    "        text = self.texts[item]#, self.text_column].compute()\n",
    "        tags = self.labels[item]#, self.label_column].compute()\n",
    "        ids = []\n",
    "        target_tag = []\n",
    "\n",
    "        for i, s in enumerate(text):\n",
    "            inputs = self.tokenizer.encode(\n",
    "                s,\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "            input_len = len(inputs)\n",
    "            ids.extend(inputs)\n",
    "            target_tag.extend([tags[i]] * input_len)\n",
    "\n",
    "        ids = ids[:self.max_len - 2]\n",
    "        target_tag = target_tag[:self.max_len - 2]\n",
    "\n",
    "        ids = [101] + ids + [102]\n",
    "        target_tag = [0] + target_tag + [0]\n",
    "\n",
    "        mask = [1] * len(ids)\n",
    "        token_type_ids = [0] * len(ids)\n",
    "\n",
    "        padding_len = self.max_len - len(ids)\n",
    "\n",
    "        ids = ids + ([0] * padding_len)\n",
    "        mask = mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        target_tag = target_tag + ([0] * padding_len)\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "class EntityModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_tag: int,\n",
    "                 model_name: str\n",
    "                 ):\n",
    "        super(EntityModel, self).__init__()\n",
    "        self.num_tag = num_tag\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.bert_drop_1 = nn.Dropout(0.3)\n",
    "        self.out_tag = nn.Linear(768, self.num_tag)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _loss_fn(output, target, mask, num_labels):\n",
    "        lfn = nn.CrossEntropyLoss()\n",
    "        active_loss = mask.view(-1) == 1\n",
    "        active_logits = output.view(-1, num_labels)\n",
    "        active_labels = torch.where(\n",
    "            active_loss,\n",
    "            target.view(-1),\n",
    "            torch.tensor(lfn.ignore_index).type_as(target)\n",
    "        )\n",
    "        loss = lfn(active_logits, active_labels)\n",
    "        return loss\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            ids,\n",
    "            mask,\n",
    "            token_type_ids,\n",
    "            target_tag\n",
    "    ):\n",
    "        o1, _ = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        bo_tag = self.bert_drop_1(o1)\n",
    "        tag = self.out_tag(bo_tag)\n",
    "\n",
    "        loss_tag = self._loss_fn(tag, target_tag, mask, self.num_tag)\n",
    "\n",
    "        return tag, loss_tag\n",
    "    \n",
    "def get_tokens(sentence):\n",
    "    \"\"\"return tokens of sentence\"\"\"\n",
    "    nlp = English()\n",
    "    tokenizer = Tokenizer(nlp.vocab)\n",
    "    return [token.text for token in tokenizer(sentence)]\n",
    "\n",
    "class ModelRunner():\n",
    "    def __init__(self):\n",
    "        self.num_ents = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def _preprocess_data(ddf, text_column, label_column):\n",
    "        def add_label(row):\n",
    "            nlp = English()\n",
    "            text = row[text_column]\n",
    "            labels = row[label_column]\n",
    "            this_doc = nlp(text)\n",
    "            result = list()\n",
    "            if isinstance(labels, list):\n",
    "                for label in labels:\n",
    "                    if getattr(label, 'label', None):\n",
    "                        ner = label.label\n",
    "                    else:\n",
    "                        ner = 'this'\n",
    "                    spans = list(filter(lambda x: x is not None,\n",
    "                                        [this_doc.char_span(int(label.start), int(label.end), label=ner)]))\n",
    "                    entities = [(span.start_char, span.end_char, ner) for span in maximal_spans(spans)]\n",
    "                    result.extend(entities)\n",
    "            return result\n",
    "\n",
    "        def span_helper(row):\n",
    "            \"convert spact entity to IOB entity for BERT\"\n",
    "            doc = nlp(row[text_column])\n",
    "            tags = biluo_to_iob(biluo_tags_from_offsets(doc, row['label1']))\n",
    "            return tags\n",
    "\n",
    "        ddf = ddf.assign(label1 = ddf.apply(add_label, axis=1, meta=list))\n",
    "        ddf = ddf.assign(label2 = ddf.apply(span_helper, axis=1, meta=list))\n",
    "        ddf = ddf.assign(text1 = ddf[text_column].apply(get_tokens, meta=list))\n",
    "        return ddf\n",
    "    \n",
    "    @staticmethod\n",
    "    def _encode_entities(ddf, label_column, meta_data):\n",
    "        def infer_labels(row):\n",
    "            unique_ents = set(row[label_column])\n",
    "            return unique_ents\n",
    "\n",
    "        def encode_labels(row, enc_tag):\n",
    "            labels = enc_tag.transform(row[label_column])\n",
    "            return labels\n",
    "\n",
    "        enc_tag = preprocessing.LabelEncoder()\n",
    "        # 1. collect all unique entities from each label in every sentence\n",
    "        res = ddf.apply(infer_labels, axis=1, meta=pd.Series())\n",
    "        unique_ents = set.union(*res.compute())\n",
    "        unique_ents = list(unique_ents)\n",
    "\n",
    "        # 2. fit a LabelEncoder to entities for neural network to receive\n",
    "        # integer labelled classes\n",
    "        _ = enc_tag.fit(unique_ents)\n",
    "        ddf = ddf.assign(label3 = ddf.apply(encode_labels, args=(enc_tag,), axis=1, meta=list))\n",
    "        num_ents = len(enc_tag.classes_)\n",
    "        meta_data.update({\n",
    "            \"enc_tag\": enc_tag,\n",
    "            \"num_ent\": num_ents\n",
    "        })\n",
    "        return ddf\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "        model.train()\n",
    "        final_loss = 0\n",
    "        for data in tqdm(data_loader, total=len(data_loader)):\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            _, loss = model(**data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            final_loss += loss.item()\n",
    "        return final_loss / len(data_loader)\n",
    "\n",
    "    @staticmethod\n",
    "    def _val_fn(data_loader, model, device):\n",
    "        model.eval()\n",
    "        final_loss = 0\n",
    "        for data in tqdm(data_loader, total=len(data_loader)):\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(device)\n",
    "            _, loss = model(**data)\n",
    "            final_loss += loss.item()\n",
    "        return final_loss / len(data_loader)\n",
    "\n",
    "    def _create_datasets(self, ddf, text_column, label_column, max_len, tokenizer, \n",
    "                         train_batch_size, valid_batch_size):\n",
    "        train_ddf, valid_ddf = train_test_split(ddf, random_state=42, test_size=0.1)\n",
    "\n",
    "        train_dataset = EntityDataset(\n",
    "            train_ddf,\n",
    "            text_column=text_column,\n",
    "            label_column=label_column,\n",
    "            max_len=max_len,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        valid_dataset = EntityDataset(\n",
    "            valid_ddf,\n",
    "            text_column=text_column,\n",
    "            label_column=label_column,\n",
    "            max_len=max_len,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        train_data_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=train_batch_size, num_workers=4\n",
    "        )\n",
    "        valid_data_loader = torch.utils.data.DataLoader(\n",
    "            valid_dataset, batch_size=valid_batch_size, num_workers=1\n",
    "        )\n",
    "        return train_ddf, valid_ddf, train_data_loader, valid_data_loader\n",
    "\n",
    "    def train(\n",
    "            self,\n",
    "            ddf: dd,\n",
    "            text_column: str,\n",
    "            label_column: str,\n",
    "            path_to_model: str,\n",
    "            iterations: int = 3,\n",
    "            config: dict = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param ddf: Dask dataframe\n",
    "        :param text_column: name of the column that contains the text to classify\n",
    "        :param label_column: name of the column that contains labels\n",
    "        :param config: config for BERT NER\n",
    "        \"\"\"\n",
    "        # Initialization\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        max_len = config.get('max_len', 128)\n",
    "        model_name = config.get('model_name', 'bert-base-uncased')\n",
    "        train_batch_size = config.get('train_batch_size', 32)\n",
    "        valid_batch_size = config.get('valid_batch_size', 8)\n",
    "        path_to_model = Path(path_to_model)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        meta_data = deepcopy(config)\n",
    "        meta_data.update({\n",
    "            'text_column': text_column,\n",
    "            'label_column': label_column\n",
    "        })\n",
    "        # Build data\n",
    "        ddf = self._preprocess_data(ddf, text_column, label_column)\n",
    "        ddf = ddf.persist()\n",
    "        ddf = self._encode_entities(ddf, 'label2', meta_data=meta_data)\n",
    "        ddf = ddf.persist()\n",
    "        train_ddf, valid_ddf, train_data_loader, valid_data_loader = self._create_datasets(ddf, 'text1',\n",
    "                                                                                           'label3', max_len,\n",
    "                                                                                           tokenizer,\n",
    "                                                                                           train_batch_size,\n",
    "                                                                                           valid_batch_size)\n",
    "        # Call Model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = EntityModel(num_tag=meta_data.get('num_ent'), model_name=model_name)\n",
    "        model.to(device)\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(\n",
    "                        nd in n for nd in no_decay\n",
    "                    )\n",
    "                ],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(\n",
    "                        nd in n for nd in no_decay\n",
    "                    )\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            }\n",
    "        ]\n",
    "        num_train_steps = int(\n",
    "            len(train_ddf) / train_batch_size * iterations\n",
    "        )\n",
    "        optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_train_steps\n",
    "        )\n",
    "        best_loss = np.inf\n",
    "        for epoch in range(iterations):\n",
    "            train_loss = self._train_fn(\n",
    "                train_data_loader,\n",
    "                model,\n",
    "                optimizer,\n",
    "                device,\n",
    "                scheduler\n",
    "            )\n",
    "            test_loss = self._val_fn(\n",
    "                valid_data_loader,\n",
    "                model,\n",
    "                device\n",
    "            )\n",
    "            print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
    "            if test_loss < best_loss:\n",
    "                torch.save(model.state_dict(), path_to_model)\n",
    "                best_loss = test_loss\n",
    "        joblib.dump(meta_data, f\"{path_to_model.parent}/meta.bin\")\n",
    "\n",
    "    @classmethod\n",
    "    def run(cls,\n",
    "            text,\n",
    "            path_to_model\n",
    "            ):\n",
    "        path_to_model = Path(path_to_model)\n",
    "        meta_data = joblib.load(f\"{path_to_model.parent}/meta.bin\")\n",
    "        enc_tag = meta_data[\"enc_tag\"]\n",
    "        model_name = meta_data['model_name']\n",
    "        num_ent = meta_data['num_ent']\n",
    "        max_len = meta_data['max_len']\n",
    "        text_column = meta_data.get('text_column', 'Word')\n",
    "        label_column = meta_data.get('label_column', 'ner')\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        num_tag = len(list(enc_tag.classes_))\n",
    "\n",
    "        tokenized_sentence = tokenizer.encode(text)\n",
    "        decoded_tokenized_sentence = tokenizer.decode(tokenized_sentence)\n",
    "\n",
    "        print(tokenized_sentence)\n",
    "        print(decoded_tokenized_sentence)\n",
    "\n",
    "        d = {text_column: [text], label_column: [None]}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        ddf = dd.from_pandas(df, npartitions=20)\n",
    "        ddf = cls._preprocess_data(ddf, text_column, label_column)\n",
    "        ddf = cls._encode_entities(ddf, 'label2', meta_data=meta_data)\n",
    "        test_dataset = EntityDataset(\n",
    "            ddf,\n",
    "            text_column='text1',\n",
    "            label_column='label3',\n",
    "            max_len=max_len,\n",
    "            tokenizer=tokenizer,\n",
    "            predict_mode=True\n",
    "            )\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         device = torch.device('cpu')\n",
    "        model = EntityModel(num_tag=num_ent, model_name=model_name)\n",
    "        model.load_state_dict(torch.load(path_to_model))\n",
    "        model.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            data = test_dataset[0]\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(device).unsqueeze(0)\n",
    "            tag, _ = model(**data)\n",
    "            \n",
    "            enc_tag_dict = dict(zip(enc_tag.transform(enc_tag.classes_), enc_tag.classes_,))\n",
    "            enc_tag_dict.update({\n",
    "                num_ent: 'O'\n",
    "            })\n",
    "            #\n",
    "            tags = list(map(lambda x: enc_tag_dict.get(x),\n",
    "                           tag.argmax(2).cpu().numpy().reshape(-1)[:len(tokenized_sentence)]))\n",
    "        \n",
    "        entities = []\n",
    "        ignore_labels = ['O']\n",
    "        print(tags)\n",
    "        for ind, (tok, ent) in enumerate(zip(tokenized_sentence, tags[:len(tokenized_sentence)])):\n",
    "            if ent not in ignore_labels:\n",
    "                entity = {\n",
    "                            \"word\": tokenizer.convert_ids_to_tokens(tok),\n",
    "                            \"entity\": ent,\n",
    "                            \"index\": ind,\n",
    "                        }\n",
    "\n",
    "                entities += [entity]\n",
    "        return group_entities(entities, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T20:08:42.615112Z",
     "start_time": "2020-09-08T20:08:42.145813Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del ddf\n",
    "except:\n",
    "    pass\n",
    "ddf = dd.from_pandas(df[:500], npartitions=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T20:22:54.722343Z",
     "start_time": "2020-09-08T20:08:52.529185Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbassi/anaconda3/envs/deepnlpb/lib/python3.6/site-packages/ipykernel_launcher.py:238: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/sbassi/anaconda3/envs/deepnlpb/lib/python3.6/site-packages/dask_ml/model_selection/_split.py:469: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  category=FutureWarning,\n",
      "100%|██████████| 14/14 [04:36<00:00, 19.77s/it]\n",
      "100%|██████████| 9/9 [00:21<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 1.6947388734136308 Valid Loss = 1.0927788946363661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:08<00:00,  1.61it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.9419715659958976 Valid Loss = 0.843152317735884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:08<00:00,  1.63it/s]\n",
      "100%|██████████| 9/9 [00:01<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.75580923472132 Valid Loss = 0.7456969453228844\n"
     ]
    }
   ],
   "source": [
    "runner = ModelRunner()\n",
    "runner.train(ddf, 'Word', 'AnnotationLabeled', config.get('path_to_model'), config.get('iterations'), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T21:41:20.802967Z",
     "start_time": "2020-09-08T21:41:01.768644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2144, 2059, 1010, 4614, 2031, 2218, 2270, 7012, 1997, 1996, 5496, 1998, 2699, 2000, 14785, 4697, 8777, 4584, 2306, 1996, 2231, 1012, 102]\n",
      "[CLS] since then, authorities have held public trials of the accused and tried to marginalize moderate officials within the government. [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbassi/anaconda3/envs/deepnlpb/lib/python3.6/site-packages/ipykernel_launcher.py:238: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-art', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-art']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'B-art', 'word': '[CLS]'},\n",
       " {'entity_group': 'B-art', 'word': '[SEP]'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelRunner.run(df['Word'].values[-2], config.get('path_to_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T22:42:57.548619Z",
     "start_time": "2020-09-02T22:42:57.545299Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T23:01:29.554763Z",
     "start_time": "2020-09-01T23:01:16.371340Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text = \"\"\"\n",
    "# The 2018–19 influenza season was a moderate severity season with two waves of influenza A \n",
    "# activity of similar magnitude during the season: A(H1N1)pdm09 predominated from October 2018 \n",
    "# to mid-February 2019, and A(H3N2) activity increased from mid-February through mid-May.\n",
    "# \"\"\"\n",
    "text = \"\"\"Mr. Trump’s tweets began just moments after a Fox News report \n",
    "by Mike Tobin, a reporter for the network, about protests in Minnesota and elsewhere.\"\"\"\n",
    "       \n",
    "         \n",
    "ModelRunner.run(text, config.get('path_to_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T15:50:49.144644Z",
     "start_time": "2020-08-26T15:50:49.129466Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
